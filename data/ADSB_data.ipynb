{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-30T14:04:13.607537300Z",
     "start_time": "2023-06-30T14:04:13.499435500Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import connectorx as cx\n",
    "from atfm.preprocess import preprocess\n",
    "from atfm.utils import map_RWY, map_corridors\n",
    "from atfm.utils import check_eligible\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_unit_vectors(flt_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the unit vectors of the trajectory DataFrame.\n",
    "    Input:\n",
    "        flt_df: A DataFrame containing the trajectory data of a single flight.\n",
    "    Output:\n",
    "        unit_df: A DataFrame containing the unit vectors of the trajectory.\n",
    "    \"\"\"\n",
    "    diff_df = flt_df.diff().dropna() # Create a new DataFrame for the differences\n",
    "    norms = np.sqrt((diff_df**2).sum(axis=1)) # Calculate the norm of each difference vector\n",
    "    unit_df = diff_df.div(norms, axis=0) # Divide each difference vector by its norm to get the unit vectors\n",
    "    unit_df.columns = ['u_x', 'u_y', 'u_z'] # The unit vectors are now stored in unit_df as 'x', 'y', and 'z'\n",
    "\n",
    "    return unit_df\n",
    "\n",
    "def is_smooth(df, threshold):\n",
    "    \"\"\"\n",
    "    Check if the trajectory is smooth.\n",
    "    Input:\n",
    "        df: A DataFrame containing the unit vectors of the trajectory.\n",
    "        threshold: The maximum allowed change in direction angle.\n",
    "    Output:\n",
    "        True if the trajectory is smooth, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the unit vector columns to a numpy array\n",
    "    unit_vectors = df[['u_x', 'u_y', 'u_z']].values\n",
    "\n",
    "    # Calculate the dot product between consecutive unit vectors\n",
    "    dot_products = np.einsum('ij, ij->i', unit_vectors[:-1], unit_vectors[1:])\n",
    "\n",
    "    # Calculate the change in direction angles\n",
    "    direction_changes = np.arccos(np.clip(dot_products, -1.0, 1.0)) * 180 / np.pi\n",
    "\n",
    "    # Apply modulo operation to keep angles within 360 degrees\n",
    "    direction_changes = direction_changes % 360\n",
    "\n",
    "    # Check if any direction change exceeds the threshold\n",
    "    if np.max(direction_changes) <= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T14:04:15.115450800Z",
     "start_time": "2023-06-30T14:04:15.062353800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(10735, 13934)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_url = 'mysql://lics:aelics070@143.248.69.46:13306/atfm_new'\n",
    "id_tab = 'flight'\n",
    "ADSB_tab = 'trajectory'\n",
    "too_short = 500\n",
    "too_long = 2000\n",
    "\n",
    "sample_frac = 0.025\n",
    "ids_arr = cx.read_sql(db_url, \"SELECT DISTINCT id FROM %s WHERE ori_length>=%d AND ori_length<=%d AND arrival=1\" % (id_tab, too_short, too_long), return_type=\"arrow\")\n",
    "ids_arr = ids_arr.to_pandas(split_blocks=False, date_as_object=False).dropna().sample(frac=sample_frac)\n",
    "ADSB_arr = cx.read_sql(db_url, f\"SELECT * FROM %s WHERE flight_id IN ({', '.join(map(str, ids_arr.values.T.tolist()[0]))});\" % (ADSB_tab), return_type=\"arrow\")\n",
    "ADSB_arr = ADSB_arr.to_pandas(split_blocks=False, date_as_object=False).dropna()\n",
    "ADSB_arr['time'] = pd.to_datetime(ADSB_arr['time'], unit='s')\n",
    "\n",
    "sample_frac = 0.05\n",
    "ids_dep = cx.read_sql(db_url, \"SELECT DISTINCT id FROM %s WHERE ori_length>=%d AND ori_length<=%d AND arrival=0\" % (id_tab, too_short, too_long), return_type=\"arrow\")\n",
    "ids_dep = ids_dep.to_pandas(split_blocks=False, date_as_object=False).dropna().sample(frac=sample_frac)\n",
    "ADSB_dep = cx.read_sql(db_url, f\"SELECT * FROM %s WHERE flight_id IN ({', '.join(map(str, ids_dep.values.T.tolist()[0]))});\" % (ADSB_tab), return_type=\"arrow\")\n",
    "ADSB_dep = ADSB_dep.to_pandas(split_blocks=False, date_as_object=False).dropna()\n",
    "ADSB_dep['time'] = pd.to_datetime(ADSB_dep['time'], unit='s')\n",
    "\n",
    "ids_arr.shape[0], ids_dep.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-30T14:04:16.820547300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "icn_lat, icn_lon, icn_alt = 37.49491667, 126.43033333, 8.0\n",
    "min_alt_change = 2000 / 3.281 # meters\n",
    "min_FAF_baro = 1600 / 3.281 # meters\n",
    "app_sector_rad = 25 # nautical miles\n",
    "target_length = too_long + 1\n",
    "\n",
    "arr_dep = []\n",
    "for ADSB in [ADSB_arr, ADSB_dep]:\n",
    "    num_reject = 0\n",
    "    df_ls = []\n",
    "    for id in tqdm(set(ADSB['flight_id'].values.tolist())):\n",
    "\n",
    "        flt_df = ADSB.loc[ADSB['flight_id'] == id]\n",
    "        flt_df = flt_df.set_index('time')\n",
    "\n",
    "        if not check_eligible(flt_df, min_alt_change, min_FAF_baro, icn_lat, icn_lon, app_sector_rad, alt_col='baroaltitude'):\n",
    "            num_reject += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            flt_df = preprocess(flt_df, ref_lat=icn_lat, ref_lon=icn_lon, ref_alt=icn_alt, periods=target_length, max_range_x=150, max_range_y=150, alt_column='baroaltitude')\n",
    "            flt_df = flt_df.dropna()\n",
    "        except Exception:\n",
    "            num_reject += 1\n",
    "            continue\n",
    "\n",
    "        unit_df = calculate_unit_vectors(flt_df)\n",
    "        if not is_smooth(unit_df, 90):\n",
    "            num_reject += 1\n",
    "            continue\n",
    "\n",
    "        joined_df = pd.concat([flt_df, unit_df], axis=1)\n",
    "        joined_df[unit_df.columns] = joined_df[unit_df.columns].shift(-1)\n",
    "        df_ls.append(joined_df)\n",
    "\n",
    "    print('Rejected %d flights' % num_reject)\n",
    "    arr_dep.append(df_ls) # arr_dep[0] is arrival, arr_dep[1] is departure"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_size = 2000\n",
    "\n",
    "for df_ls in arr_dep:\n",
    "\n",
    "    if len(df_ls) < data_size:\n",
    "        print('Not enough data')\n",
    "        continue\n",
    "\n",
    "    random.seed(42)  # Set a common seed value for consistent sampling\n",
    "\n",
    "    flt_traj_and_path_ls = [df.dropna().T.to_numpy() for df in df_ls if df.iloc[:, -3:].dropna(how='all').T.shape == (3, target_length-1)]\n",
    "    sampled_data = random.sample(flt_traj_and_path_ls, data_size)\n",
    "\n",
    "    flt_traj_ls = [data[:3, :] for data in sampled_data]\n",
    "    flt_traj_array = np.stack(flt_traj_ls, axis=0)\n",
    "\n",
    "    flt_path_ls = [data[-3:, :] for data in sampled_data]\n",
    "    flt_path_array = np.stack(flt_path_ls, axis=0)\n",
    "\n",
    "    n_train = int(len(flt_path_array) * 0.8)\n",
    "    train_data = flt_path_array[:n_train]\n",
    "    test_data = flt_path_array[n_train:]\n",
    "    train_traj = flt_traj_array[:n_train]\n",
    "    test_traj = flt_traj_array[n_train:]\n",
    "\n",
    "    print(\"Data for Arrival\" if df_ls is arr_dep[0] else \"Data for Departure\")\n",
    "    print(\"Flight Path Dataset Shape ====> \\tTrainset: \", train_data.shape, \"\\tTestset: \", test_data.shape)\n",
    "    print(\"Reference Trajectory Shape ====> \\tTrainset: \", train_traj.shape, \"\\tTestset: \", test_traj.shape)\n",
    "\n",
    "    ## Save signals to file\n",
    "    data_dir = './ADSB_data_arr' if df_ls is arr_dep[0] else './ADSB_data_dep'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    with open(data_dir + '/x_train.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(data_dir + '/x_test.pkl', 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    with open(data_dir + '/traj_train.pkl', 'wb') as f:\n",
    "        pickle.dump(train_traj, f)\n",
    "    with open(data_dir + '/traj_test.pkl', 'wb') as f:\n",
    "        pickle.dump(test_traj, f)\n",
    "\n",
    "    ## Plot test data\n",
    "    fig, axs = plt.subplots(6, figsize=(10, 15))\n",
    "    # x plots from test_traj\n",
    "    for i in range(3):\n",
    "        axs[i].plot(range(len(test_traj)), test_traj[:, i])\n",
    "    axs[0].set_ylabel('X')\n",
    "    axs[1].set_ylabel('Y')\n",
    "    axs[2].set_ylabel('Z')\n",
    "\n",
    "    # unit vector plot from test_data\n",
    "    for i in range(3):\n",
    "        axs[i+3].plot(range(len(test_data)), test_data[:, i])\n",
    "    axs[3].set_ylabel('U_X')\n",
    "    axs[4].set_ylabel('U_Y')\n",
    "    axs[5].set_ylabel('U_Z')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "atfm",
   "language": "python",
   "display_name": "ATFM"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
